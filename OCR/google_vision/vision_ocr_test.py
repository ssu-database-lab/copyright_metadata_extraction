import io
import os
import json
import shutil
from google.cloud import vision
from google.protobuf.json_format import MessageToDict  # JSON ë³€í™˜ ëª¨ë“ˆ

os.environ["GRPC_DNS_RESOLVER"] = "native"

# âœ… 1. ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ì„¤ì • (Colab í™˜ê²½)
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "conversational_AI_order_agent/semiotic-pager-466612-t0-c587b9296fb8.json"
# path to the IMAGE folder
image_folder = "images"

# âœ… 2. Vision API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = vision.ImageAnnotatorClient()

# âœ… 3. OCR ì ìš©í•  ì´ë¯¸ì§€ ê²½ë¡œ
# getting the image from input and navigate to the directory
image_path = input("Enter the image path: ")  # âœ… OCR ì ìš©í•  ì´ë¯¸ì§€
# save the image to the directory images/   
os.makedirs(image_folder, exist_ok=True)
# save the image to the directory images/
shutil.copy(image_path, image_folder)

# navigate to the directory
os.chdir(image_folder) 

# âœ… 4. ì´ë¯¸ì§€ ë¡œë“œ
with io.open(image_path, "rb") as image_file:
    content = image_file.read()
image = vision.Image(content=content)


# âœ… 5. OCR ìš”ì²­ (ë¬¸ì„œ ë‚´ í…ìŠ¤íŠ¸ ê²€ì¶œ)
response = client.document_text_detection(image=image)

# âœ… 6. OCR ê²°ê³¼ ì¶”ì¶œ ë° ì¶œë ¥
if response.text_annotations:
    extracted_text = response.text_annotations[0].description  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    print("\nğŸ”¹ Extracted OCR Text:\n")
    print(extracted_text)

    # âœ… 7. JSON ë³€í™˜ í›„ ì €ì¥ (ğŸš¨ ê¸°ì¡´ ì˜¤ë¥˜ í•´ê²°)
    response_dict = MessageToDict(response._pb)  # âœ… `_pb` ì‚¬ìš©í•˜ì—¬ ë³€í™˜

    # âœ… 8. OCR ê²°ê³¼ ì €ì¥ (í…ìŠ¤íŠ¸)
    with open("ocr_result.txt", "w", encoding="utf-8") as text_file:
        text_file.write(extracted_text)
    print(f"\nâœ… OCR í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: ocr_result.txt")

    # âœ… 9. OCR ì „ì²´ ê²°ê³¼ ì €ì¥ (JSON)
    with open("ocr_result.json", "w", encoding="utf-8") as json_file:
        json.dump(response_dict, json_file, indent=4, ensure_ascii=False)
    print(f"\nâœ… OCR ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: ocr_result.json")

else:
    print("\nâŒ OCR ê²°ê³¼ ì—†ìŒ")