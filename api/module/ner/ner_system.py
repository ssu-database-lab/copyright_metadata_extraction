#!/usr/bin/env python3
"""
í†µí•© NER ì‹œìŠ¤í…œ - ì™„ì „ ë…ë¦½ ë²„ì „
ner_train.pyì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ NER ì‹œìŠ¤í…œ êµ¬ì„±
- ê°•í™”ëœ ì´ì¤‘ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (B-I-O ëª¨ë¸ + ì •ê·œí‘œí˜„ì‹)
- ìë™ ëª¨ë¸ í›ˆë ¨ ê¸°ëŠ¥
- ë†’ì€ ì •í™•ë„ ë³´ì¥
- ê°„ë‹¨í•œ API ì¸í„°í˜ì´ìŠ¤
"""

import os
import sys
import json
import csv
import time
import logging
import re
import subprocess
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any, Optional, Set, Tuple
from tqdm import tqdm

# PyTorch ë° Transformers
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
import warnings
warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# ì—”í‹°í‹° íƒ€ì… ì •ì˜ (ner_train.pyì™€ ë™ì¼)
ENTITY_TYPES = [
    "NAME",           # ì´ë¦„
    "PHONE",          # ì „í™”ë²ˆí˜¸
    "ADDRESS",        # ì£¼ì†Œ
    "DATE",           # ë‚ ì§œ
    "COMPANY",        # íšŒì‚¬/ê¸°ê´€ëª…
    "EMAIL",          # ì´ë©”ì¼
    "POSITION",       # ì§ì±…/ì§ìœ„
    "CONTRACT_TYPE",  # ê³„ì•½ì„œ ìœ í˜•
    "CONSENT_TYPE",   # ë™ì˜ì„œ ìœ í˜•
    "RIGHT_INFO",     # ê¶Œë¦¬ì •ë³´
    "MONEY",          # ê¸ˆì•¡
    "PERIOD",         # ê¸°ê°„
    "PROJECT_NAME",   # ì‚¬ì—…ëª…
    "LAW_REFERENCE",  # ë²•ë ¹ ê·¼ê±°
    "ID_NUM",         # ì‹ ë¶„ì¦ë²ˆí˜¸
    "TITLE",          # ì œëª©
    "URL",            # URLì •ë³´
    "DESCRIPTION",    # ì„¤ëª…
    "TYPE",           # ìœ í˜•
    "STATUS",         # ìƒíƒœ
    "DEPARTMENT",     # ë¶€ì„œì •ë³´
    "LANGUAGE",       # ì–¸ì–´
    "QUANTITY"        # ìˆ˜ëŸ‰ì •ë³´
]

# ê¸°ë³¸ ì„¤ì •
DEFAULT_MAX_LENGTH = 512

# model_config.jsonì—ì„œ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
def load_default_model_name():
    """model_config.jsonì—ì„œ ê¸°ë³¸ ëª¨ë¸ ì´ë¦„ ë¡œë“œ"""
    try:
        config_path = Path(__file__).parent.parent.parent / "model_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            default_model = config.get("ner", {}).get("default_model", "klue-roberta-large")
            print(f"âœ“ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •: {default_model}")
            return default_model
    except Exception as e:
        print(f"âš ï¸  model_config.json ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
    return "klue-roberta-large"

DEFAULT_MODEL_NAME = load_default_model_name()

def check_system_requirements(verbose: bool = False):
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if verbose:
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name} ({memory_gb:.1f}GB)")
        else:
            print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    return device

def get_model_path(model_name: str = DEFAULT_MODEL_NAME) -> Path:
    """
    ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë°˜í™˜
    
    ê²½ë¡œ êµ¬ì¡°: api/models/ner/{model_name}/
    ì˜ˆ: api/models/ner/klue-roberta-large/
    """
    current_dir = Path(__file__).parent
    api_dir = current_dir.parent.parent
    
    # ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡°: models/ner/{model_name}
    models_base_dir = api_dir / "models" / "ner"
    models_base_dir.mkdir(parents=True, exist_ok=True)
    
    # ëª¨ë¸ëª…ì—ì„œ ìŠ¬ë˜ì‹œë¥¼ ëŒ€ì‹œë¡œ ë³€ê²½ (ì˜ˆ: klue/roberta-large -> klue-roberta-large)
    model_name_safe = model_name.replace('/', '-')
    
    model_path = models_base_dir / model_name_safe
    return model_path

def load_model_and_tokenizer(model_path: Path, verbose: bool = True):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    if verbose:
        print(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(str(model_path))
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForTokenClassification.from_pretrained(str(model_path))
    
    # ë¼ë²¨ ë§µ ë¡œë“œ
    label_map_file = model_path / "label_map.json"
    if label_map_file.exists():
        with open(label_map_file, 'r', encoding='utf-8') as f:
            label_info = json.load(f)
        id2label = label_info['id2label']
        id2label = {int(k): v for k, v in id2label.items()}
    else:
        # ê¸°ë³¸ ë¼ë²¨ ë§µ ìƒì„±
        labels = ["O"]
        for entity in ENTITY_TYPES:
            labels.extend([f"B-{entity}", f"I-{entity}"])
        id2label = {i: label for i, label in enumerate(labels)}
    
    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ ì´ë™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    if verbose:
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({device})")
    return tokenizer, model, id2label, device

def extract_entities_by_bio_tagging(text: str, tokenizer, model, id2label: dict, device) -> Set[Tuple[str, str]]:
    """B-I-O íƒœê¹… ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
    entities = set()
    
    # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
    sentences = split_text_smartly(text, DEFAULT_MAX_LENGTH)
    
    for sentence in sentences:
        if len(sentence.strip()) < 3:
            continue
            
        try:
            # í† í°í™”
            encoding = tokenizer(
                sentence,
                return_tensors="pt",
                truncation=True,
                max_length=DEFAULT_MAX_LENGTH,
                add_special_tokens=True,
                return_offsets_mapping=True,
                padding=True
            )
            
            input_ids = encoding["input_ids"].to(device)
            attention_mask = encoding["attention_mask"].to(device)
            offset_mapping = encoding["offset_mapping"][0]
            
            # ëª¨ë¸ ì˜ˆì¸¡
            with torch.no_grad():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                predictions = torch.softmax(outputs.logits, dim=-1)
                predicted_labels = torch.argmax(predictions, dim=-1).squeeze(0).tolist()
                confidence_scores = torch.max(predictions, dim=-1)[0].squeeze(0).tolist()
            
            # B-I-O íƒœê¹…ìœ¼ë¡œ ì—”í‹°í‹° ì¶”ì¶œ
            current_entity = ""
            current_type = None
            current_start = -1
            current_confidence = 0.0
            
            for token_idx, (pred_id, confidence, (start, end)) in enumerate(zip(predicted_labels, confidence_scores, offset_mapping)):
                if start == 0 and end == 0:  # íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°
                    continue
                
                pred_label = id2label.get(pred_id, 'O')
                
                # ë” ë†’ì€ ì‹ ë¢°ë„ë§Œ ì‚¬ìš© (ë…¸ì´ì¦ˆ ê°ì†Œ)
                if confidence < 0.7:
                    pred_label = 'O'
                
                if pred_label.startswith('B-'):
                    # ì´ì „ ì—”í‹°í‹° ì €ì¥
                    if current_entity and current_type and current_confidence > 0.75:
                        clean_entity = clean_entity_text(current_entity)
                        if is_valid_entity(clean_entity, current_type):
                            entities.add((clean_entity, current_type))
                    
                    # ìƒˆ ì—”í‹°í‹° ì‹œì‘
                    current_entity = sentence[start:end]
                    current_type = pred_label[2:]
                    current_start = start
                    current_confidence = confidence
                    
                elif pred_label.startswith('I-') and current_type == pred_label[2:]:
                    # í˜„ì¬ ì—”í‹°í‹° í™•ì¥
                    if current_start != -1:
                        current_entity = sentence[current_start:end]
                        current_confidence = min(current_confidence, confidence)
                else:
                    # ì—”í‹°í‹° ì¢…ë£Œ
                    if current_entity and current_type and current_confidence > 0.75:
                        clean_entity = clean_entity_text(current_entity)
                        if is_valid_entity(clean_entity, current_type):
                            entities.add((clean_entity, current_type))
                    
                    current_entity = ""
                    current_type = None
                    current_start = -1
                    current_confidence = 0.0
            
            # ë§ˆì§€ë§‰ ì—”í‹°í‹° ì²˜ë¦¬
            if current_entity and current_type and current_confidence > 0.75:
                clean_entity = clean_entity_text(current_entity)
                if is_valid_entity(clean_entity, current_type):
                    entities.add((clean_entity, current_type))
                    
        except Exception as e:
            logger.warning(f"ë¬¸ì¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    return entities

def extract_entities_by_regex(text: str) -> Set[Tuple[str, str]]:
    """ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ë°±ì—… ì—”í‹°í‹° ì¶”ì¶œ"""
    entities = set()
    
    # ì´ë¦„ íŒ¨í„´ (í•œêµ­ì–´ ì´ë¦„)
    name_patterns = [
        r'[ê°€-í£]{2,4}(?=\s*(?:ì”¨|ë‹˜|ì„ ìƒ|êµìˆ˜|ë°•ì‚¬|ì˜ì›|ëŒ€í‘œ|ì´ì‚¬|ë¶€ì¥|ê³¼ì¥|ëŒ€ë¦¬|ì£¼ì„|íŒ€ì¥))',
        r'ì„±ëª…:\s*([ê°€-í£]{2,4})',
        r'ì´ë¦„:\s*([ê°€-í£]{2,4})',
        r'ê³„ì•½ì:\s*([ê°€-í£]{2,4})',
        r'(?:ê°‘|ì„):\s*([ê°€-í£]{2,4})'
    ]
    
    for pattern in name_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            name = match.group(1) if match.groups() else match.group(0)
            name = name.replace(':', '').strip()
            if len(name) >= 2 and name.isalpha() and is_valid_entity(name, 'NAME'):
                entities.add((name, 'NAME'))
    
    # ì „í™”ë²ˆí˜¸ íŒ¨í„´
    phone_patterns = [
        r'(\d{2,3}-\d{3,4}-\d{4})',
        r'(\d{3}-\d{4}-\d{4})',
        r'ì „í™”ë²ˆí˜¸:\s*([0-9-]{10,15})',
        r'ì—°ë½ì²˜:\s*([0-9-]{10,15})',
        r'TEL:\s*([0-9-]{10,15})'
    ]
    
    for pattern in phone_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            phone = match.group(1) if match.groups() else match.group(0)
            phone = phone.replace('ì „í™”ë²ˆí˜¸:', '').replace('ì—°ë½ì²˜:', '').replace('TEL:', '').strip()
            # ì „í™”ë²ˆí˜¸ ìœ íš¨ì„± ê°•í™”
            if re.match(r'^[0-9-]{10,15}$', phone) and is_valid_entity(phone, 'PHONE'):
                entities.add((phone, 'PHONE'))
    
    # ì£¼ì†Œ íŒ¨í„´
    address_patterns = [
        r'ì£¼ì†Œ:\s*([ê°€-í£0-9\s-]+(?:ì‹œ|êµ¬|êµ°|ë™|ë¡œ|ê¸¸)[ê°€-í£0-9\s-]*)',
        r'([ê°€-í£]+(?:ì‹œ|ë„)\s+[ê°€-í£]+(?:êµ¬|êµ°)\s+[ê°€-í£0-9\s-]*(?:ë¡œ|ê¸¸|ë™)[\s0-9]*)',
        r'(ì„œìš¸ì‹œ\s+[ê°€-í£]+êµ¬[ê°€-í£0-9\s-]*)',
        r'(ë¶€ì‚°ì‹œ\s+[ê°€-í£]+êµ¬[ê°€-í£0-9\s-]*)'
    ]
    
    for pattern in address_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            address = match.group(1) if match.groups() else match.group(0)
            address = address.replace('ì£¼ì†Œ:', '').strip()
            if len(address) > 5 and is_valid_entity(address, 'ADDRESS'):
                entities.add((address, 'ADDRESS'))
    
    # ì´ë©”ì¼ íŒ¨í„´
    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
    matches = re.finditer(email_pattern, text)
    for match in matches:
        email = match.group(1)
        entities.add((email, 'EMAIL'))
    
    # íšŒì‚¬ëª… íŒ¨í„´
    company_patterns = [
        r'([ê°€-í£]+(?:ì£¼ì‹íšŒì‚¬|ãˆœ|íšŒì‚¬|ê¸°ê´€|ì¬ë‹¨|í˜‘íšŒ|ì„¼í„°|ì—°êµ¬ì†Œ|ì—°êµ¬ì›|ëŒ€í•™êµ|ì²­|ì²˜|ë¶€|ì›))',
        r'ì†Œì†:\s*([ê°€-í£0-9\s]+(?:ì£¼ì‹íšŒì‚¬|ãˆœ|íšŒì‚¬|ê¸°ê´€|ì¬ë‹¨|í˜‘íšŒ|ì„¼í„°|ì—°êµ¬ì†Œ|ì—°êµ¬ì›|ëŒ€í•™êµ|ì²­|ì²˜|ë¶€|ì›))',
        r'íšŒì‚¬:\s*([ê°€-í£0-9\s]+)',
        r'ê¸°ê´€:\s*([ê°€-í£0-9\s]+)'
    ]
    
    for pattern in company_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            company = match.group(1) if match.groups() else match.group(0)
            company = company.replace('ì†Œì†:', '').replace('íšŒì‚¬:', '').replace('ê¸°ê´€:', '').strip()
            if len(company) > 2 and is_valid_entity(company, 'COMPANY'):
                entities.add((company, 'COMPANY'))
    
    # ë‚ ì§œ íŒ¨í„´
    date_patterns = [
        r'(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)',
        r'(\d{4}\.\d{1,2}\.\d{1,2})',
        r'(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{1,2}/\d{1,2}/\d{4})',
        r'ì‘ì„±ì¼:\s*([0-9ë…„ì›”ì¼.\s-]+)',
        r'ê³„ì•½ì¼:\s*([0-9ë…„ì›”ì¼.\s-]+)'
    ]
    
    for pattern in date_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            date = match.group(1) if match.groups() else match.group(0)
            date = date.replace('ì‘ì„±ì¼:', '').replace('ê³„ì•½ì¼:', '').strip()
            if len(date) > 4 and is_valid_entity(date, 'DATE'):
                entities.add((date, 'DATE'))
    
    # ê¸ˆì•¡ íŒ¨í„´
    money_patterns = [
        r'(\d{1,3}(?:,\d{3})*ì›)',
        r'ê¸ˆ\s*(\d{1,3}(?:,\d{3})*ì›)',
        r'ê³„ì•½ê¸ˆ:\s*([0-9,ì›\s]+)',
        r'ì‚¬ì—…ë¹„:\s*([0-9,ì›\s]+)'
    ]
    
    for pattern in money_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            money = match.group(1) if match.groups() else match.group(0)
            money = money.replace('ê¸ˆ', '').replace('ê³„ì•½ê¸ˆ:', '').replace('ì‚¬ì—…ë¹„:', '').strip()
            if 'ì›' in money and len(money) > 2:
                entities.add((money, 'MONEY'))
    
    return entities

def split_text_smartly(text: str, max_length: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„í• """
    if len(text) <= max_length:
        return [text]
    
    sentences = []
    text_sentences = text.replace('\n', '. ').split('.')
    
    current_chunk = ""
    for sentence in text_sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk + sentence) <= max_length:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                sentences.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        sentences.append(current_chunk.strip())
    
    return sentences

def clean_entity_text(entity: str) -> str:
    """ì—”í‹°í‹° í…ìŠ¤íŠ¸ ì •ë¦¬"""
    entity = entity.strip()
    entity = re.sub(r'^[:\s,.-]+', '', entity)
    entity = re.sub(r'[:\s,.-]+$', '', entity)
    return entity

def group_entities_by_type(entities: List[Tuple[str, str]]) -> Dict[str, List[str]]:
    """
    ì—”í‹°í‹°ë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    
    Args:
        entities: [(ê°’, íƒ€ì…), ...] í˜•íƒœì˜ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        {íƒ€ì…: [ê°’1, ê°’2, ...], ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    grouped = {}
    for entity, entity_type in entities:
        if entity_type not in grouped:
            grouped[entity_type] = []
        grouped[entity_type].append(entity)
    
    # ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
    return dict(sorted(grouped.items()))

def is_valid_entity(entity: str, entity_type: Optional[str] = None) -> bool:
    """ìœ íš¨í•œ ì—”í‹°í‹°ì¸ì§€ í™•ì¸ - ì •í™•ë„ ê°œì„  ë²„ì „"""
    
    # 1. ê¸°ë³¸ ê¸¸ì´ ì²´í¬ (ìµœì†Œ 2ì)
    if len(entity) < 2:
        return False
    
    # 2. ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì—¬ëŸ¬ ì¤„ ë°©ì§€)
    if '\n' in entity:
        line_count = entity.count('\n')
        # ADDRESSëŠ” ìµœëŒ€ 1ê°œ ì¤„ë°”ê¿ˆ, ë‚˜ë¨¸ì§€ëŠ” 2ê°œê¹Œì§€ í—ˆìš©
        if entity_type == 'ADDRESS':
            if line_count > 1:  # ADDRESSëŠ” 1ì¤„ê¹Œì§€ë§Œ
                return False
        elif line_count > 2:  # ë‹¤ë¥¸ íƒ€ì…ì€ 2ì¤„ê¹Œì§€
            return False
    if len(entity) > 50:  # ìµœëŒ€ 50ì
        return False
    
    # 3. ë¶ˆí•„ìš”í•œ ë¬¸ìë“¤ ì œì™¸
    invalid_chars = ['â–¡', 'â˜‘', 'â—‹', 'â—']
    if any(char in entity for char in invalid_chars):
        return False
    
    # 4. ìˆ«ìë§Œìœ¼ë¡œëŠ” ì•ˆë¨
    if entity.isdigit():
        return False
    
    # 5. ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ ì œì™¸ (ì˜ˆ: "100ì›", "2024ë…„")
    if re.match(r'^\d+[ë…„ì›”ì¼ì›]$', entity):
        return False
    
    # 6. ë¶ˆì™„ì „í•œ ë‹¨ì–´ ì œì™¸ (ë§ˆì¹¨í‘œ, ì‰¼í‘œë¡œ ëë‚¨)
    if entity.endswith(('.', ',', 'Â·', ')', '(', ':')):
        return False
    if entity.startswith(('.', ',', 'Â·', ')', '(')):
        return False
    
    # 7. ì¡°ì‚¬/ì ‘ë¯¸ì‚¬ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
    josa_list = ['ì„', 'ë¥¼', 'ê°€', 'ëŠ”', 'ì€', 'ì˜', 'ì´', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ìœ¼ë¡œ', 'ë¡œ', 'ê³¼', 'ì™€', 'ë°']
    if entity in josa_list:
        return False
    
    # 8. íƒ€ì…ë³„ íŠ¹ìˆ˜ ê²€ì¦
    if entity_type:
        # NAME íƒ€ì… ê²€ì¦
        if entity_type == 'NAME':
            # ë™ì‚¬í˜• ì œì™¸
            if entity in ['ì–‘ë„', 'ì–‘ìˆ˜', 'ì œê³µ', 'ìˆ˜ë ¹', 'ëŒ€í‘œ', 'ë‹´ë‹¹', 'ê´€ë¦¬', 'ì €ì‘', 'íšŒì‚¬', 'ê¸°ê´€']:
                return False
            # ì—­í•  ë‹¨ì–´ ì œì™¸ (ëì´ ì/ì¸/ì²˜)
            if entity.endswith(('ì', 'ì¸', 'ì²˜')) and len(entity) <= 4:
                # ì‹¤ì œ ì´ë¦„ ì˜ˆì™¸
                if entity not in ['ê¹€ì', 'ì´ì', 'ë°•ì¸', 'í•œì']:
                    return False
            # ì—­í•  ë‹¨ì–´ ì™„ì „ ë§¤ì¹­
            role_words = ['ì–‘ë„ì', 'ì–‘ë„ì¸', 'ì–‘ìˆ˜ì¸', 'ì–‘ìˆ˜ì', 'ì œê³µì', 'ì´ìš©ì', 'ìˆ˜ë ¹ì', 'ìˆ˜ë ¹ì¸']
            if entity in role_words:
                return False
            # ì¤‘ê°„ì— ë§ˆì¹¨í‘œ/ì‰¼í‘œ ìˆëŠ” ê²½ìš° ì œì™¸ (ì˜ˆ: "ì–‘ë„. ì")
            if '.' in entity or ',' in entity:
                return False
        
        # COMPANY íƒ€ì… ê²€ì¦
        elif entity_type == 'COMPANY':
            # ì¼ë°˜ ëª…ì‚¬ ì œì™¸
            general_nouns = ['ì—°ë½ì²˜', 'ì£¼ì†Œ', 'ì„±ëª…', 'ì „í™”', 'íœ´ëŒ€', 'ë‹´ë‹¹', 'íšŒì‚¬', 'ê¸°ê´€', 
                           'ê³µê³µê¸°ê´€', 'ë°©ì†¡ì‚¬', 'ê´€í• ë²•ì›', 'ë³´ì „ì²˜', 'ê´€í• ', 'ë³´ì „', 'í™•ì¸',
                           'ìˆ˜í–‰ìë¡œë¶€', 'ì €ì‘ì¸ì ‘ê¶Œìë¡œë¶€', 'ì €ì‘ê¶Œìë¡œë¶€', 'ê¶Œë¦¬ìë¡œë¶€']
            if entity in general_nouns:
                return False
            # ë¶€ë¶„ ë¬¸ìì—´ ì²´í¬ (ì¼ë°˜ ëª…ì‚¬ í¬í•¨ ì œì™¸)
            if 'ì—°ë½ì²˜' in entity or 'ì£¼ì†Œ' in entity:
                return False
            # "~ë¡œë¶€"ë¡œ ëë‚˜ëŠ” ê²½ìš° ì œì™¸
            if entity.endswith('ë¡œë¶€'):
                return False
            # "~ë²•ì›" í¬í•¨ ì œì™¸ (ê´€í• ë²•ì›, ì§€ë°©ë²•ì› ë“±)
            if 'ë²•ì›' in entity and len(entity) <= 6:
                return False
            # "~ì²˜" ë¡œ ëë‚˜ëŠ” ê´€ì²­ ëª…ì‚¬ ì œì™¸
            if entity.endswith('ì²˜') and len(entity) <= 3:
                return False
            # ë„ˆë¬´ ì§§ì€ íšŒì‚¬ëª… ì œì™¸ (4ì ë¯¸ë§Œ, íŠ¹ìˆ˜ ì˜ˆì™¸ ì œì™¸)
            if len(entity) < 4 and entity not in ['KBS', 'MBC', 'SBS', 'EBS']:
                return False
        
        # PHONE íƒ€ì… ê²€ì¦
        elif entity_type == 'PHONE':
            # ë‚ ì§œ íŒ¨í„´ ì œì™¸ (ì˜ˆ: "2020. 6. 20")
            if re.match(r'\d{4}\.\s*\d{1,2}\.\s*\d{1,2}', entity):
                return False
            # ì§§ì€ ìˆ«ì ì œì™¸ (7ìë¦¬ ë¯¸ë§Œ, ì‹¤ì œ ì „í™”ë²ˆí˜¸ëŠ” ë³´í†µ 7-11ìë¦¬)
            digits_only = ''.join(c for c in entity if c.isdigit())
            if len(digits_only) < 7:
                return False
            # ì „í™”ë²ˆí˜¸ ë’¤ì— ë‚ ì§œê°€ ë¶™ì€ ê²½ìš° ì œì™¸
            if re.search(r'\d{2,4}\.\s*\d{4}', entity):
                return False
        
        # POSITION íƒ€ì… ê²€ì¦
        elif entity_type == 'POSITION':
            invalid_positions = ['ì €ì‘', 'íšŒì‚¬', 'ì‚¬ì—…', 'ìŠ¤íŠœë””ì˜¤', 'ëŒ€í‘œ', 'ë‹´ë‹¹', 'ìƒëŒ€ë°©', 'ê´€ê³„', 'ì €ì‘ë¬¼']
            if entity in invalid_positions:
                return False
        
        # DESCRIPTION íƒ€ì… ê²€ì¦
        elif entity_type == 'DESCRIPTION':
            # ë„ˆë¬´ ì§§ì€ ì„¤ëª… ì œì™¸
            if len(entity) < 5:
                return False
            # ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            generic_words = ['ê³µê³µ', 'ì €ì‘ë¬¼', 'ì €ì‘ê¶Œ', 'ì‚¬íšŒí†µë…ìƒ', 'êµ­ë¯¼ë“¤ì´', 'ì˜ë¦¬ì ìœ¼ë¡œë„ ì´ìš©í•  ìˆ˜',
                           'ê³µê³µì €ì‘ë¬¼', 'ì €ì‘ë¬¼ì„', 'ì €ì‘ë¬¼ ì œì‘ì„', 'ì €ì‘ë¬¼ì˜']
            if entity in generic_words:
                return False
            # í¬í•¨ ì²´í¬
            if 'ê³µê³µì €ì‘ë¬¼' in entity and 'ëª©ì ìœ¼ë¡œ' in entity:  # "ê³µê³µì €ì‘ë¬¼ ì œì‘ì„ ëª©ì ìœ¼ë¡œ" ê°™ì€ ê²ƒ
                return False
            # "~ì„/ë¥¼/ê°€/ëŠ”"ìœ¼ë¡œ ëë‚˜ëŠ” ë¶ˆì™„ì „í•œ ì„¤ëª… ì œì™¸
            if entity.endswith(('ì„', 'ë¥¼', 'ê°€', 'ëŠ”', 'ì´', 'ì˜', 'ìœ¼ë¡œ')):
                return False
            # ì¤‘ê°„ì— ë§ˆì¹¨í‘œê°€ ìˆëŠ” ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œì™¸ (ì˜ˆ: "ê³µê³µì €ì‘ë¬¼ì„ ììœ ë¡­. ê²Œ")
            if '. ' in entity or ' .' in entity:
                return False
        
        # DATE íƒ€ì… ê²€ì¦
        elif entity_type == 'DATE':
            # íŒŒì¼ í™•ì¥ì ì œì™¸
            if entity.lower() in ['png', 'jpg', 'pdf', 'txt', 'doc', 'xlsx', 'jpeg', 'gif']:
                return False
        
        # CONSENT_TYPE, CONTRACT_TYPE íƒ€ì… ê²€ì¦
        elif entity_type in ['CONSENT_TYPE', 'CONTRACT_TYPE']:
            # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸ (ìµœì†Œ 3ì)
            if len(entity) < 3:
                return False
            # ë¶ˆì™„ì „í•œ ë‹¨ì–´ ì œì™¸ (ì˜ˆ: "í™•ì¸ ë°")
            if entity.endswith(' ë°') or entity.endswith(' ì™€') or entity.endswith(' ë˜ëŠ”'):
                return False
        
        # ADDRESS íƒ€ì… ê²€ì¦
        elif entity_type == 'ADDRESS':
            # ì¤„ë°”ê¿ˆì´ 2ê°œ ì´ìƒì¸ ê²½ìš° ì œì™¸ (ë„ˆë¬´ ê¸´ ì£¼ì†Œ)
            if entity.count('\n') > 1:
                return False
    
    # 9. ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
    if entity.strip() == '':
        return False
    
    return True

def download_pretrained_model(model_name: str, model_path: Path, verbose: bool = True) -> bool:
    """
    Hugging Faceì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_name: Hugging Face ëª¨ë¸ ì´ë¦„ (ì˜ˆ: klue/roberta-large, xlm-roberta-large)
        model_path: ì €ì¥í•  ë¡œì»¬ ê²½ë¡œ
        verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        bool: ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€
    """
    try:
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ”½ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            print(f"   ëª¨ë¸: {model_name}")
            print(f"   ì €ì¥ ê²½ë¡œ: {model_path}")
            print(f"{'='*60}")
        
        # ëª¨ë¸ëª… ì •ê·œí™” (íŒŒì¼ëª… í˜•ì‹ì„ Hugging Face í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        # klue-roberta-large -> klue/roberta-large
        # xlm-roberta-large -> xlm-roberta-large (ê·¸ëŒ€ë¡œ)
        hf_model_name = model_name
        if model_name.startswith('klue-'):
            hf_model_name = model_name.replace('klue-', 'klue/', 1)
        elif model_name.startswith('bert-'):
            hf_model_name = model_name  # bert-base-multilingual-cased ë“±ì€ ê·¸ëŒ€ë¡œ
        
        print(f"ğŸ“¥ Hugging Face ëª¨ë¸ëª…: {hf_model_name}")
        
        # Hugging Faceì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        
        print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘... ({hf_model_name})")
        tokenizer = AutoTokenizer.from_pretrained(hf_model_name)
        
        print(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ({hf_model_name})")
        # NERìš© ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ ë ˆì´ë¸”ë¡œ ì´ˆê¸°í™”)
        model = AutoModelForTokenClassification.from_pretrained(
            hf_model_name,
            num_labels=len(ENTITY_TYPES) * 2 + 1,  # B-I-O íƒœê¹…
            ignore_mismatched_sizes=True
        )
        
        # ë¡œì»¬ì— ì €ì¥
        model_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘... ({model_path})")
        tokenizer.save_pretrained(str(model_path))
        model.save_pretrained(str(model_path))
        
        # ë¼ë²¨ ë§¤í•‘ ì €ì¥
        labels = ["O"]
        for entity in ENTITY_TYPES:
            labels.extend([f"B-{entity}", f"I-{entity}"])
        
        id2label = {i: label for i, label in enumerate(labels)}
        label2id = {label: i for i, label in enumerate(labels)}
        
        label_info = {
            'id2label': {str(k): v for k, v in id2label.items()},
            'label2id': label2id,
            'entity_types': ENTITY_TYPES
        }
        
        label_map_file = model_path / "label_map.json"
        with open(label_map_file, 'w', encoding='utf-8') as f:
            json.dump(label_info, f, ensure_ascii=False, indent=2)
        
        if verbose:
            print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print(f"   - config.json: {(model_path / 'config.json').exists()}")
            print(f"   - model.safetensors: {(model_path / 'model.safetensors').exists()}")
            print(f"   - tokenizer.json: {(model_path / 'tokenizer.json').exists()}")
            print(f"   - label_map.json: {label_map_file.exists()}")
            print(f"{'='*60}\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def train_model_if_needed(model_name: str, model_path: Path, verbose: bool = True, auto_train: bool = False, auto_download: bool = True) -> bool:
    """
    í•„ìš”ì‹œ ëª¨ë¸ í›ˆë ¨ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„ (Hugging Face ëª¨ë¸ëª…)
        model_path: ëª¨ë¸ ê²½ë¡œ
        verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        auto_train: ìë™ í›ˆë ¨ í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        auto_download: Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        bool: ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    if model_path.exists() and (model_path / "config.json").exists():
        if verbose:
            print("âœ“ ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return True
    
    # 1ë‹¨ê³„: Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„ (auto_download=Trueì¸ ê²½ìš°)
    if auto_download:
        print(f"âš ï¸  ë¡œì»¬ì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print(f"ğŸ” Hugging Faceì—ì„œ '{model_name}' ëª¨ë¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        
        if download_pretrained_model(model_name, model_path, verbose):
            print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! Fine-tuning ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return True
        else:
            print(f"âš ï¸  Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2ë‹¨ê³„: ìë™ í›ˆë ¨ ì‹œë„ (auto_train=Trueì¸ ê²½ìš°)
    if not auto_train:
        print(f"âš ï¸  ìë™ í›ˆë ¨ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print(f"âš ï¸  ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print(f"   1) auto_download=Trueë¡œ ì„¤ì •í•˜ì—¬ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ")
        print(f"   2) auto_train=Trueë¡œ ì„¤ì •í•˜ì—¬ ìë™ í›ˆë ¨")
        print(f"   3) ìˆ˜ë™ í›ˆë ¨: python api/module/ner/ner_train.py")
        return False
    
    # ìë™ í›ˆë ¨ ì‹¤í–‰
    print("ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        # ner_train.py ì‹¤í–‰
        current_dir = Path(__file__).parent
        train_script = current_dir / "ner_train.py"
        
        if not train_script.exists():
            print(f"í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_script}")
            return False
        
        print(f"í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: {train_script}")
        
        # subprocessë¡œ í›ˆë ¨ ì‹¤í–‰
        process = subprocess.Popen([
            sys.executable, str(train_script)
        ], cwd=str(current_dir), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
        text=True, universal_newlines=True)
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ í‘œì‹œ
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        
        return_code = process.poll()
        
        if return_code == 0:
            print("ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
        else:
            print(f"ëª¨ë¸ í›ˆë ¨ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì½”ë“œ: {return_code})")
            return False
            
    except Exception as e:
        print(f"ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def extract_entities_from_text(text: str, model_name: Optional[str] = None, debug: bool = False, train: bool = False) -> List[Tuple[str, str]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (í†µí•© ë©”ì¸ í•¨ìˆ˜)
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        model_name: ëª¨ë¸ ì´ë¦„
        debug: ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
        train: Trueì´ë©´ ë¬´ì¡°ê±´ ëª¨ë¸ í›ˆë ¨ í›„ ì˜ˆì¸¡ (ê¸°ë³¸ê°’: False)
    """
    if debug:
        print(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì)")
    
    # ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    if model_name is None:
        model_name = DEFAULT_MODEL_NAME
        if debug:
            print(f"ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    
    if debug:
        print(f"ì‚¬ìš© ëª¨ë¸: {model_name}")
    
    all_entities = set()
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = get_model_path(model_name)
    if debug:
        print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    # train=Trueì´ë©´ ë¬´ì¡°ê±´ í›ˆë ¨
    if train:
        if debug:
            print("=" * 60)
            print("train=True: ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("=" * 60)
        
        # ner_train í•¨ìˆ˜ í˜¸ì¶œ (debug íŒŒë¼ë¯¸í„° ì „ë‹¬)
        result = ner_train(
            model_name=model_name,
            epochs=3,
            force_retrain=True,
            debug=debug
        )
        
        if not result.get('success', False):
            if debug:
                print(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                print(f"ì •ê·œí‘œí˜„ì‹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            if debug:
                print(f"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    else:
        # train=Falseì´ë©´ ê¸°ì¡´ ëª¨ë¸ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì •ê·œí‘œí˜„ì‹ë§Œ
        model_exists = model_path.exists() and (model_path / "config.json").exists()
        if not model_exists and debug:
            print(f"ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print(f"ì •ê·œí‘œí˜„ì‹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"train=Trueë¡œ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.")
    
    try:
        # 1. B-I-O ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡
        model_exists = model_path.exists() and (model_path / "config.json").exists()
        
        if model_exists:
            if debug:
                print(f"íŒŒì¸íŠœë‹ ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨: {model_path}")
                print("B-I-O íƒœê¹… ê¸°ë°˜ ì˜ˆì¸¡ ì‹œì‘...")
            
            tokenizer, model, id2label, device = load_model_and_tokenizer(model_path, verbose=debug)
            if debug:
                print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - ë¼ë²¨ ìˆ˜: {len(id2label)}ê°œ")
            
            bio_entities = extract_entities_by_bio_tagging(text, tokenizer, model, id2label, device)
            all_entities.update(bio_entities)
            
            if debug:
                print(f"B-I-O ì˜ˆì¸¡ ê²°ê³¼: {len(bio_entities)}ê°œ ì—”í‹°í‹°")
        
        else:
            if debug:
                print(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                print(f"   - ë””ë ‰í† ë¦¬ ì¡´ì¬: {model_path.exists()}")
                if model_path.exists():
                    print(f"   - config.json ì¡´ì¬: {(model_path / 'config.json').exists()}")
        
    except Exception as e:
        if debug:
            print(f"ëª¨ë¸ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    # 2. ì •ê·œí‘œí˜„ì‹ ë°±ì—… ì˜ˆì¸¡
    if debug:
        print("ì •ê·œí‘œí˜„ì‹ ë°±ì—… ì˜ˆì¸¡ ì‹œì‘...")
    
    regex_entities = extract_entities_by_regex(text)
    all_entities.update(regex_entities)
    
    if debug:
        print(f"ì •ê·œí‘œí˜„ì‹ ì˜ˆì¸¡ ê²°ê³¼: {len(regex_entities)}ê°œ ì—”í‹°í‹°")
    
    # 3. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
    final_entities = []
    seen_entities = set()
    
    for entity, label in all_entities:
        entity_lower = entity.lower().strip()
        if entity_lower not in seen_entities:
            seen_entities.add(entity_lower)
            final_entities.append((entity, label))
    
    # ì—”í‹°í‹° íƒ€ì…ë³„ë¡œ ì •ë ¬
    final_entities.sort(key=lambda x: (x[1], x[0]))
    
    if debug:
        print(f"ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼: {len(final_entities)}ê°œ ì—”í‹°í‹°")
        for entity, label in final_entities[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            print(f"  - {entity} ({label})")
    
    return final_entities

def ner_predict(
    input_path: str,
    output_path: str,
    model_name: Optional[str] = None,
    confidence_threshold: float = 0.85,
    output_format: str = "both",
    save_statistics: bool = True,
    entity_filter: Optional[List[str]] = None,
    train: bool = False,
    debug: bool = False
) -> Dict[str, Any]:
    """
    ë””ë ‰í† ë¦¬ ë˜ëŠ” íŒŒì¼ì— ëŒ€í•œ NER ì˜ˆì¸¡ ìˆ˜í–‰
    
    Args:
        input_path: ì…ë ¥ íŒŒì¼/ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_path: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: model_config.jsonì˜ default_model)
        confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        output_format: ì¶œë ¥ í˜•ì‹
        save_statistics: í†µê³„ ì €ì¥ ì—¬ë¶€
        entity_filter: ì¶”ì¶œí•  ì—”í‹°í‹° íƒ€ì… í•„í„°
        train: Trueì´ë©´ ë¬´ì¡°ê±´ ëª¨ë¸ í›ˆë ¨ í›„ ì˜ˆì¸¡ (ê¸°ë³¸ê°’: False)
        debug: Trueì´ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸ê°’: False)
    
    Returns:
        Dict[str, Any]: ì˜ˆì¸¡ ê²°ê³¼ ì •ë³´
    """
    start_time = time.time()
    
    # ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    if model_name is None:
        model_name = DEFAULT_MODEL_NAME
        if debug:
            print(f"âš ï¸  ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {model_name}")
    
    try:
        if debug:
            print("=" * 60)
            print("NER ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹œì‘")  
            print("=" * 60)
        
        # 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        device = check_system_requirements(verbose=debug)
        
        # 2. ì…ë ¥ ê²½ë¡œ í™•ì¸
        input_path_obj = Path(input_path)
        output_path_obj = Path(output_path)
        
        if not input_path_obj.exists():
            return {
                "success": False,
                "error": f"ì…ë ¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path_obj}"
            }
        
        # 3. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ëª¨ë¸ë³„ë¡œ ë¶„ë¦¬)
        # ner/{model_name} ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì €ì¥
        # ëª¨ë¸ ì´ë¦„ì—ì„œ ìŠ¬ë˜ì‹œë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½ (íŒŒì¼ì‹œìŠ¤í…œ í˜¸í™˜)
        model_dir_name = model_name.replace('/', '-')
        ner_dir = output_path_obj / "ner" / model_dir_name
        ner_dir.mkdir(parents=True, exist_ok=True)
        
        if debug:
            print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {ner_dir}")
            print(f"ëª¨ë¸: {model_name}")
        
        # 4. ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìƒì„±
        files_to_process = []
        
        if input_path_obj.is_file():
            if input_path_obj.suffix.lower() in ['.txt', '.md']:
                files_to_process = [input_path_obj]
        else:
            # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸°
            for ext in ['*.txt', '*.md']:
                files_to_process.extend(input_path_obj.glob(f"**/{ext}"))
        
        if not files_to_process:
            return {
                "success": False,
                "error": "ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        if debug:
            print(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(files_to_process)}")
        
        # 5. ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘
        if debug:
            print("ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘...")
        
        all_entities = []
        processed_files = 0
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”ëŠ” í•­ìƒ í‘œì‹œ (disable=False)
        import sys
        for file_path in tqdm(files_to_process, desc="íŒŒì¼ ì²˜ë¦¬ ì¤‘", disable=False, file=sys.stdout, ncols=80):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if len(content.strip()) < 10:
                    continue
                
                # ì—”í‹°í‹° ì¶”ì¶œ (train, debug íŒŒë¼ë¯¸í„° ì „ë‹¬)
                entities = extract_entities_from_text(content, model_name=model_name, debug=False, train=train)
                
                # ê²°ê³¼ ì €ì¥ - ì…ë ¥ ê²½ë¡œ êµ¬ì¡° ìœ ì§€ (pdf_to_imageì™€ ë™ì¼í•œ íŒ¨í„´)
                if entities:
                    # ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
                    entities_grouped = group_entities_by_type(entities)
                    
                    # ê²°ê³¼ êµ¬ì¡°: íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”ëœ í˜•íƒœ
                    file_result = {
                        'file': str(file_path),
                        'entities': entities_grouped,
                        'entity_count': len(entities),
                        'entity_types': list(entities_grouped.keys())
                    }
                    all_entities.extend(entities)
                    
                    # ì…ë ¥ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°í•˜ì—¬ ì¶œë ¥ êµ¬ì¡° ìƒì„±
                    file_path_obj = Path(file_path)
                    
                    if input_path_obj.is_file():
                        # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš° - íŒŒì¼ëª…ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±í•˜ì§€ ì•Šê³  ì§ì ‘ ì €ì¥
                        result_file = ner_dir / f"{file_path_obj.stem}_entities.json"
                    else:
                        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° - input_pathë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ìƒëŒ€ ê²½ë¡œ êµ¬ì¡° ìœ ì§€
                        try:
                            relative_path = file_path_obj.relative_to(input_path_obj)
                            # ìƒëŒ€ ê²½ë¡œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ _entities.json ì¶”ê°€
                            if relative_path.parent != Path('.'):
                                result_dir = ner_dir / relative_path.parent
                                result_dir.mkdir(parents=True, exist_ok=True)
                                result_file = result_dir / f"{file_path_obj.stem}_entities.json"
                            else:
                                result_file = ner_dir / f"{file_path_obj.stem}_entities.json"
                        except ValueError:
                            # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° ì‹¤íŒ¨ì‹œ í´ë°±
                            result_file = ner_dir / f"{file_path_obj.stem}_entities.json"
                    
                    # JSON íŒŒì¼ ì €ì¥
                    with open(result_file, 'w', encoding='utf-8') as f:
                        json.dump(file_result, f, ensure_ascii=False, indent=2)
                
                processed_files += 1
                
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                continue
        
        # 6. ì „ì²´ ê²°ê³¼ ìš”ì•½
        entity_stats = defaultdict(int)
        for entity, entity_type in all_entities:
            entity_stats[entity_type] += 1
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'total_files_processed': processed_files,
            'total_entities_found': len(all_entities),
            'unique_entities': len(set(all_entities)),
            'entity_types_count': dict(entity_stats),
            'processing_time': time.time() - start_time,
            'timestamp': timestamp
        }
        
        # 7. ìš”ì•½ íŒŒì¼ ì €ì¥
        summary_file = ner_dir / "summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nNER ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}/{len(files_to_process)}")
        print(f"ì¶”ì¶œëœ ì—”í‹°í‹°: {len(all_entities)}ê°œ")
        print(f"ê²°ê³¼ ì €ì¥: {ner_dir}")
        print(f"ì†Œìš” ì‹œê°„: {summary['processing_time']:.1f}ì´ˆ")
        
        return {
            "success": True,
            "processed_files": processed_files,
            "total_entities": len(all_entities),
            "extracted_entities": all_entities,
            "statistics": summary,
            "output_directory": str(ner_dir),
            "processing_time": summary['processing_time'],
            "summary_file": str(summary_file)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

def ner_train(
    epochs: int = 3,
    batch_size: int = 8,
    learning_rate: float = 3e-5,
    model_name: str = DEFAULT_MODEL_NAME,
    output_dir: Optional[str] = None,
    enable_fp16: bool = True,
    max_length: int = 128,
    warmup_steps: int = 100,
    save_steps: int = 200,
    eval_steps: int = 100,
    force_retrain: bool = False,
    callback_url: Optional[str] = None,
    debug: bool = False
) -> Dict[str, Any]:
    """
    NER ëª¨ë¸ í›ˆë ¨ API
    
    Args:
        epochs: í›ˆë ¨ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)
        learning_rate: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 3e-5)
        model_name: ê¸°ë³¸ ëª¨ë¸ëª… (ê¸°ë³¸ê°’: klue-roberta-large)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)
        enable_fp16: Mixed precision training ì‚¬ìš© ì—¬ë¶€
        max_length: ìµœëŒ€ í† í° ê¸¸ì´
        warmup_steps: Warmup ìŠ¤í… ìˆ˜
        save_steps: ëª¨ë¸ ì €ì¥ ê°„ê²©
        eval_steps: í‰ê°€ ê°„ê²©
        force_retrain: ê¸°ì¡´ ëª¨ë¸ì´ ìˆì–´ë„ ì¬í›ˆë ¨ ì—¬ë¶€
        callback_url: í›ˆë ¨ ìƒíƒœ ì½œë°± URL (ì˜µì…˜)
        debug: Trueì´ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸ê°’: False)
    
    Returns:
        Dict[str, Any]: í›ˆë ¨ ê²°ê³¼ ì •ë³´
    """
    start_time = time.time()
    
    try:
        if debug:
            print("=" * 60)
            print("NER ëª¨ë¸ í›ˆë ¨ API ì‹œì‘")
            print("=" * 60)
        
        # 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        device = check_system_requirements(verbose=debug)
        
        # 2. ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        if output_dir:
            model_path = Path(output_dir)
        else:
            model_path = get_model_path(model_name)
        
        # 3. ê¸°ì¡´ ëª¨ë¸ í™•ì¸
        if not force_retrain and model_path.exists() and (model_path / "config.json").exists():
            return {
                "success": True,
                "message": "ê¸°ì¡´ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                "model_path": str(model_path),
                "training_time": 0,
                "skipped": True
            }
        
        if debug:
            print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_path}")
            print(f"í›ˆë ¨ ì„¤ì •:")
            print(f"  - Epochs: {epochs}")
            print(f"  - Batch Size: {batch_size}")
            print(f"  - Learning Rate: {learning_rate}")
            print(f"  - Max Length: {max_length}")
            print(f"  - FP16: {enable_fp16}")
        
        # 4. ner_train.py ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        current_dir = Path(__file__).parent
        train_script = current_dir / "ner_train.py"
        
        if not train_script.exists():
            return {
                "success": False,
                "error": f"í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_script}"
            }
        
        # 5. í›ˆë ¨ ëª…ë ¹ì–´ êµ¬ì„±
        cmd_args = [
            sys.executable, str(train_script),
            "--epochs", str(epochs),
            "--batch_size", str(batch_size),
            "--learning_rate", str(learning_rate),
            "--model_name", model_name,
            "--output_dir", str(model_path),
            "--max_length", str(max_length),
            "--warmup_steps", str(warmup_steps),
            "--save_steps", str(save_steps),
            "--eval_steps", str(eval_steps)
        ]
        
        if enable_fp16:
            cmd_args.append("--fp16")
        
        if debug:
            print(f"í›ˆë ¨ ì‹œì‘...")
        
        # 6. í›ˆë ¨ ì‹¤í–‰
        process = subprocess.Popen(
            cmd_args,
            cwd=str(current_dir),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            universal_newlines=True
        )
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ ë° ì§„í–‰ ìƒí™© ì¶”ì 
        training_logs = []
        current_epoch = 0
        current_step = 0
        total_steps = 0
        
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
                
            if output:
                line = output.strip()
                print(line)
                training_logs.append(line)
                
                # ì§„í–‰ ìƒí™© íŒŒì‹±
                if "Epoch" in line and "/" in line:
                    try:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if "Epoch" in part and i + 1 < len(parts):
                                epoch_info = parts[i + 1]
                                if "/" in epoch_info:
                                    current_epoch = int(epoch_info.split("/")[0])
                    except:
                        pass
                
                if "Step" in line and "/" in line:
                    try:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if "Step" in part and i + 1 < len(parts):
                                step_info = parts[i + 1]
                                if "/" in step_info:
                                    current_step, total_steps = map(int, step_info.split("/"))
                    except:
                        pass
                
                # ì½œë°± URLì´ ìˆìœ¼ë©´ ì§„í–‰ ìƒí™© ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ì‹œ)
                if callback_url and current_step > 0:
                    progress = {
                        "epoch": current_epoch,
                        "step": current_step,
                        "total_steps": total_steps,
                        "progress_percent": (current_step / max(total_steps, 1)) * 100
                    }
                    # ì—¬ê¸°ì„œ callback_urlë¡œ POST ìš”ì²­ ë³´ë‚¼ ìˆ˜ ìˆìŒ
        
        return_code = process.poll()
        training_time = time.time() - start_time
        
        # 7. ê²°ê³¼ ì²˜ë¦¬
        if return_code == 0:
            # í›ˆë ¨ëœ ëª¨ë¸ ê²€ì¦
            if model_path.exists() and (model_path / "config.json").exists():
                return {
                    "success": True,
                    "message": "NER ëª¨ë¸ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
                    "model_path": str(model_path),
                    "training_time": training_time,
                    "final_epoch": current_epoch,
                    "total_steps": total_steps,
                    "training_logs": training_logs[-50:],  # ë§ˆì§€ë§‰ 50ì¤„ë§Œ ë°˜í™˜
                    "config": {
                        "epochs": epochs,
                        "batch_size": batch_size,
                        "learning_rate": learning_rate,
                        "model_name": model_name,
                        "max_length": max_length
                    }
                }
            else:
                return {
                    "success": False,
                    "error": "í›ˆë ¨ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "training_time": training_time,
                    "training_logs": training_logs[-20:]
                }
        else:
            return {
                "success": False,
                "error": f"ëª¨ë¸ í›ˆë ¨ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (Exit code: {return_code})",
                "training_time": training_time,
                "training_logs": training_logs[-20:]
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": f"í›ˆë ¨ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}",
            "training_time": time.time() - start_time
        }

def get_training_status(model_path: Optional[str] = None) -> Dict[str, Any]:
    """í›ˆë ¨ ìƒíƒœ í™•ì¸"""
    if model_path:
        check_path = Path(model_path)
    else:
        check_path = get_model_path()
    
    status = {
        "model_exists": check_path.exists(),
        "model_path": str(check_path),
        "files": []
    }
    
    if check_path.exists():
        # ëª¨ë¸ íŒŒì¼ë“¤ í™•ì¸
        important_files = ["config.json", "model.safetensors", "tokenizer.json", "label_map.json"]
        for file_name in important_files:
            file_path = check_path / file_name
            status["files"].append({
                "name": file_name,
                "exists": file_path.exists(),
                "size": file_path.stat().st_size if file_path.exists() else 0,
                "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat() if file_path.exists() else None
            })
        
        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        checkpoints = list(check_path.glob("checkpoint-*"))
        status["checkpoints"] = len(checkpoints)
        status["latest_checkpoint"] = str(max(checkpoints, key=lambda p: p.stat().st_mtime)) if checkpoints else None
    
    return status

def ner_evaluate(
    test_data_path: Optional[str] = None,
    model_name: Optional[str] = None,
    output_path: Optional[str] = None,
    verbose: bool = False,
    debug: bool = False,
    use_validation: bool = False,
    use_test: bool = False,
    max_samples: Optional[int] = None
) -> Dict[str, Any]:
    """
    NER ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (F1 Score, Precision, Recall)
    
    Args:
        test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (BIO í¬ë§· .txt íŒŒì¼)
                       Noneì´ë©´ ìë™ìœ¼ë¡œ training/{model_name}/test.txt ì‚¬ìš©
        model_name: í‰ê°€í•  ëª¨ë¸ ì´ë¦„
        output_path: í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
                    Noneì´ë©´ module/ner/validate/{model_name}/ ì— ìë™ ì €ì¥
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        debug: ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: False, verboseë³´ë‹¤ ìš°ì„ )
        use_validation: Trueë©´ validation.txt ì‚¬ìš© (í›ˆë ¨ ì¤‘ ì„±ëŠ¥ í™•ì¸ìš©)
        use_test: Trueë©´ test.txt ì‚¬ìš© (ìµœì¢… í‰ê°€ìš©)
        max_samples: í‰ê°€í•  ìµœëŒ€ ë¬¸ì¥ ìˆ˜ (Noneì´ë©´ ì „ì²´, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    
    Returns:
        Dict[str, Any]: í‰ê°€ ê²°ê³¼ (F1, Precision, Recall, ì—”í‹°í‹°ë³„ ì ìˆ˜)
    """
    # debug=Trueì´ë©´ verboseë„ Trueë¡œ ì„¤ì •
    if debug:
        verbose = True
    
    try:
        from sklearn.metrics import precision_recall_fscore_support, classification_report
    except ImportError:
        print("âš ï¸  scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "scikit-learn"])
            from sklearn.metrics import precision_recall_fscore_support, classification_report
        except Exception as e:
            return {
                "success": False,
                "error": f"scikit-learn ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}"
            }
    
    start_time = time.time()
    
    # ëª¨ë¸ ì´ë¦„ ì„¤ì •
    if model_name is None:
        model_name = DEFAULT_MODEL_NAME
    
    # ëª¨ë¸ëª… ì •ê·œí™” (klue/roberta-large â†’ klue-roberta-large)
    model_name_safe = model_name.replace('/', '-')
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent
    api_dir = current_dir.parent.parent
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ìë™ ì„¤ì •
    if test_data_path is None:
        training_dir = current_dir / "training" / model_name_safe
        
        if use_test:
            # ìµœì¢… í‰ê°€ìš© (ì ˆëŒ€ í›ˆë ¨ì— ì‚¬ìš© ì•ˆ í•¨!)
            test_data_path = str(training_dir / "test.txt")
            eval_type = "Test (ìµœì¢… í‰ê°€)"
        elif use_validation:
            # í›ˆë ¨ ì¤‘ ì„±ëŠ¥ í™•ì¸ìš©
            test_data_path = str(training_dir / "validation.txt")
            eval_type = "Validation (í›ˆë ¨ ì¤‘)"
        else:
            # ê¸°ë³¸ê°’: test.txt
            test_data_path = str(training_dir / "test.txt")
            eval_type = "Test (ìµœì¢… í‰ê°€)"
    else:
        eval_type = "Custom"
    
    # ì¶œë ¥ ê²½ë¡œ ìë™ ì„¤ì •
    if output_path is None:
        validate_dir = current_dir / "validate" / model_name_safe
        validate_dir.mkdir(parents=True, exist_ok=True)
        output_path = str(validate_dir)
    
    if verbose:
        print("=" * 60)
        print("NER ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        print(f"âœ“ ì‚¬ìš© ëª¨ë¸: {model_name}")
        print(f"âœ“ í‰ê°€ íƒ€ì…: {eval_type}")
        print(f"âœ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data_path}")
        print(f"âœ“ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_path = Path(test_data_path)
    if not test_path.exists():
        return {
            "success": False,
            "error": f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_data_path}"
        }
    
    # BIO í¬ë§· íŒŒì‹±
    true_labels = []
    pred_labels = []
    sentences = []
    current_sentence = []
    current_labels = []
    
    with open(test_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                if current_sentence:
                    sentences.append(current_sentence)
                    true_labels.extend(current_labels)
                    current_sentence = []
                    current_labels = []
                continue
            
            parts = line.split('\t')
            if len(parts) >= 2:
                token, label = parts[0], parts[1]
                current_sentence.append(token)
                current_labels.append(label)
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
    if current_sentence:
        sentences.append(current_sentence)
        true_labels.extend(current_labels)
    
    if not sentences:
        return {
            "success": False,
            "error": "í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        }
    
    # max_samples ì ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    original_sentence_count = len(sentences)
    if max_samples is not None and max_samples < len(sentences):
        # ì²˜ìŒ max_samplesê°œ ë¬¸ì¥ë§Œ ì‚¬ìš©
        sentences = sentences[:max_samples]
        # true_labelsë„ í•´ë‹¹ ë¬¸ì¥ë“¤ì˜ ë¼ë²¨ë§Œ
        true_labels = []
        for sent_labels in [s for s in sentences]:
            # ë‹¤ì‹œ íŒŒì¼ì—ì„œ í•´ë‹¹ ë¬¸ì¥ì˜ ë¼ë²¨ ì¶”ì¶œ
            pass
        # ê°„ë‹¨í•˜ê²Œ: ë‹¤ì‹œ ë¡œë“œ
        sentences_temp = []
        true_labels = []
        sentence_count = 0
        current_sentence = []
        current_labels = []
        
        with open(test_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    if current_sentence:
                        sentences_temp.append(current_sentence)
                        true_labels.extend(current_labels)
                        current_sentence = []
                        current_labels = []
                        sentence_count += 1
                        if sentence_count >= max_samples:
                            break
                    continue
                
                parts = line.split('\t')
                if len(parts) >= 2:
                    token, label = parts[0], parts[1]
                    current_sentence.append(token)
                    current_labels.append(label)
        
        sentences = sentences_temp
        
        if verbose:
            print(f"âš ï¸  ìƒ˜í”Œë§: {max_samples}/{original_sentence_count}ê°œ ë¬¸ì¥ë§Œ í‰ê°€")
    
    if verbose:
        print(f"âœ“ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìˆ˜: {len(sentences)}")
        print(f"âœ“ í…ŒìŠ¤íŠ¸ í† í° ìˆ˜: {len(true_labels)}")
    
    # 2. ëª¨ë¸ ë¡œë“œ
    model_path = get_model_path(model_name)
    model_source = "local"
    
    # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ Hugging Faceì—ì„œ ë¡œë“œ
    if not model_path.exists():
        if verbose:
            print(f"âš ï¸  ë¡œì»¬ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Hugging Faceì—ì„œ ë¡œë“œ: {model_name}")
        model_path_str = model_name  # Hugging Face ëª¨ë¸ëª… ì‚¬ìš©
        model_source = "huggingface"
    else:
        model_path_str = str(model_path)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path_str)
        model = AutoModelForTokenClassification.from_pretrained(model_path_str)
        model.to(device)
        model.eval()
        
        # label_map ë¡œë“œ (ë¡œì»¬ ëª¨ë¸ë§Œ)
        if model_source == "local":
            label_map_path = model_path / "label_map.json"
            if label_map_path.exists():
                with open(label_map_path, 'r', encoding='utf-8') as f:
                    label_data = json.load(f)
                    # id2labelì´ ì¤‘ì²©ëœ ê²½ìš° ì²˜ë¦¬
                    if 'id2label' in label_data:
                        id2label = {int(k): v for k, v in label_data['id2label'].items()}
                    else:
                        id2label = {int(k): v for k, v in label_data.items()}
            else:
                id2label = model.config.id2label
        else:
            # Hugging Face ëª¨ë¸ì€ configì—ì„œ ê°€ì ¸ì˜´
            id2label = model.config.id2label
            
        if verbose:
            print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¶œì²˜: {model_source})")
    except Exception as e:
        return {
            "success": False,
            "error": f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        }
    
    # 3. ì˜ˆì¸¡ ìˆ˜í–‰
    if verbose:
        print("ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    
    with torch.no_grad():
        for sentence in tqdm(sentences, disable=not verbose):
            # í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ ê³µë°± ì—†ì´ ë¬¸ì¥ êµ¬ì„±
            text = ''.join(sentence)
            
            # í† í°í™” with offset_mapping
            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True,
                return_offsets_mapping=True
            )
            offset_mapping = inputs.pop('offset_mapping')[0]
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ì˜ˆì¸¡
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)
            pred_label_ids = predictions[0].cpu().numpy()
            
            # Offset mappingì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì ë‹¨ìœ„ ë¼ë²¨ ì •ë ¬
            char_labels = ['O'] * len(text)
            
            for idx, (start, end) in enumerate(offset_mapping):
                if start == 0 and end == 0:  # [CLS], [SEP], [PAD]
                    continue
                
                label_id = int(pred_label_ids[idx])
                label = id2label.get(label_id, 'O')
                
                # í•´ë‹¹ offset ë²”ìœ„ì˜ ëª¨ë“  ë¬¸ìì— ë¼ë²¨ í• ë‹¹
                # ì²« ë¬¸ìëŠ” B- íƒœê·¸ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” I- íƒœê·¸ë¡œ ë³€í™˜
                for char_idx in range(start, end):
                    if char_idx < len(char_labels):
                        if char_idx == start:
                            # ì²« ë¬¸ì: B- íƒœê·¸ ìœ ì§€
                            char_labels[char_idx] = label
                        else:
                            # ë‚˜ë¨¸ì§€ ë¬¸ì: I- íƒœê·¸ë¡œ ë³€í™˜
                            if label.startswith('B-'):
                                char_labels[char_idx] = label.replace('B-', 'I-')
                            else:
                                char_labels[char_idx] = label
            
            # ì›ë³¸ í† í° ìˆ˜ì™€ ë§¤ì¹­ (sentenceëŠ” ë¬¸ì ë¦¬ìŠ¤íŠ¸)
            token_pred_labels = []
            for idx, char in enumerate(sentence):
                if idx < len(char_labels):
                    token_pred_labels.append(char_labels[idx])
                else:
                    token_pred_labels.append('O')
            
            pred_labels.extend(token_pred_labels)
    
    # 4. ì—”í‹°í‹° ë ˆë²¨ í‰ê°€ (í† í° ë‹¨ìœ„ê°€ ì•„ë‹Œ ì—”í‹°í‹° ë‹¨ìœ„)
    # ë¬¸ì¥ë³„ë¡œ ì¬êµ¬ì„±
    all_true_labels = []
    all_pred_labels = []
    
    sentence_start = 0
    for sentence in sentences:
        sentence_len = len(sentence)
        sentence_true = true_labels[sentence_start:sentence_start + sentence_len]
        sentence_pred = pred_labels[sentence_start:sentence_start + sentence_len]
        
        all_true_labels.append(sentence_true)
        all_pred_labels.append(sentence_pred)
        
        sentence_start += sentence_len
    
    # Seqevalì„ ì‚¬ìš©í•œ ì—”í‹°í‹° ë ˆë²¨ í‰ê°€
    try:
        from seqeval.metrics import precision_score as seqeval_precision
        from seqeval.metrics import recall_score as seqeval_recall
        from seqeval.metrics import f1_score as seqeval_f1
        from seqeval.metrics import classification_report
        
        precision = seqeval_precision(all_true_labels, all_pred_labels, zero_division=0)
        recall = seqeval_recall(all_true_labels, all_pred_labels, zero_division=0)
        f1 = seqeval_f1(all_true_labels, all_pred_labels, zero_division=0)
        
        # ì—”í‹°í‹°ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        entity_types = set()
        for labels in all_true_labels:
            for label in labels:
                if label != 'O':
                    # B-NAME, I-NAME â†’ NAMEìœ¼ë¡œ í†µì¼
                    entity_type = label.replace('B-', '').replace('I-', '')
                    entity_types.add(entity_type)
        
        entity_metrics = {}
        for entity_type in entity_types:
            # í•´ë‹¹ ì—”í‹°í‹°ë§Œ ì¶”ì¶œí•˜ì—¬ í‰ê°€
            filtered_true = []
            filtered_pred = []
            
            for true_sent, pred_sent in zip(all_true_labels, all_pred_labels):
                filtered_true_sent = []
                filtered_pred_sent = []
                
                for t_label, p_label in zip(true_sent, pred_sent):
                    # í•´ë‹¹ ì—”í‹°í‹° íƒ€ì…ë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” Oë¡œ
                    t_clean = t_label.replace('B-', '').replace('I-', '')
                    p_clean = p_label.replace('B-', '').replace('I-', '')
                    
                    if t_clean == entity_type:
                        filtered_true_sent.append(t_label)
                    else:
                        filtered_true_sent.append('O')
                    
                    if p_clean == entity_type:
                        filtered_pred_sent.append(p_label)
                    else:
                        filtered_pred_sent.append('O')
                
                filtered_true.append(filtered_true_sent)
                filtered_pred.append(filtered_pred_sent)
            
            try:
                p = seqeval_precision(filtered_true, filtered_pred, zero_division=0)
                r = seqeval_recall(filtered_true, filtered_pred, zero_division=0)
                f = seqeval_f1(filtered_true, filtered_pred, zero_division=0)
                
                entity_metrics[entity_type] = {
                    'precision': float(p) * 100,
                    'recall': float(r) * 100,
                    'f1_score': float(f) * 100,
                    'support': sum(1 for labels in all_true_labels for label in labels if label.replace('B-', '').replace('I-', '') == entity_type)
                }
            except:
                entity_metrics[entity_type] = {
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1_score': 0.0,
                    'support': 0
                }
    
    except ImportError:
        # Seqevalì´ ì—†ìœ¼ë©´ ê¸°ì¡´ í† í° ë ˆë²¨ í‰ê°€ ì‚¬ìš©
        if verbose:
            print("âš ï¸  seqevalì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í† í° ë ˆë²¨ í‰ê°€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels,
            pred_labels,
            average='weighted',
            zero_division=0
        )
        
        # ì—”í‹°í‹°ë³„ ë©”íŠ¸ë¦­ (í† í° ë ˆë²¨)
        entity_labels = list(set(true_labels) - {'O'})
        entity_metrics = {}
        
        for entity_type in entity_labels:
            y_true_binary = [1 if label == entity_type else 0 for label in true_labels]
            y_pred_binary = [1 if label == entity_type else 0 for label in pred_labels]
            
            p, r, f, s = precision_recall_fscore_support(
                y_true_binary,
                y_pred_binary,
                average='binary',
                zero_division=0
            )
            
            entity_metrics[entity_type] = {
                'precision': float(p) * 100 if p is not None else 0.0,
                'recall': float(r) * 100 if r is not None else 0.0,
                'f1_score': float(f) * 100 if f is not None else 0.0,
                'support': int(s) if s is not None else 0
            }
    
    # 5. ê²°ê³¼ ì¶œë ¥
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        print(f"\nì „ì²´ ì„±ëŠ¥:")
        print(f"  â€¢ Precision (ì •ë°€ë„): {precision * 100:.2f}%")
        print(f"  â€¢ Recall (ì¬í˜„ìœ¨):    {recall * 100:.2f}%")
        print(f"  â€¢ F1 Score:           {f1 * 100:.2f}%")
        print(f"  â€¢ ì´ í† í° ìˆ˜:         {len(true_labels):,}")
        
        print(f"\nì—”í‹°í‹°ë³„ ì„±ëŠ¥:")
        print("-" * 60)
        print(f"{'ì—”í‹°í‹° íƒ€ì…':<20} {'F1 Score':<12} {'Precision':<12} {'Recall':<12}")
        print("-" * 60)
        
        # F1 Score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_entities = sorted(
            entity_metrics.items(),
            key=lambda x: x[1]['f1_score'],
            reverse=True
        )
        
        for entity_type, metrics in sorted_entities:
            # B-, I- ì ‘ë‘ì‚¬ ì œê±°
            display_name = entity_type.replace('B-', '').replace('I-', '')
            print(f"{display_name:<20} {metrics['f1_score']:>10.2f}%  {metrics['precision']:>10.2f}%  {metrics['recall']:>10.2f}%")
        
        print("-" * 60)
    
    # 6. ê²°ê³¼ ì €ì¥
    results = {
        "success": True,
        "model_name": model_name,
        "test_data_path": str(test_path),
        "overall": {
            "precision": float(precision) * 100,
            "recall": float(recall) * 100,
            "f1_score": float(f1) * 100,
            "total_tokens": len(true_labels)
        },
        "entity_metrics": entity_metrics,
        "evaluation_time": time.time() - start_time
    }
    
    if output_path:
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # JSON íŒŒì¼ ì €ì¥ (module/ner/validate/{model_name}/)
        timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
        eval_prefix = "validation" if use_validation else "test" if use_test else "eval"
        json_file = output_dir / f"{eval_prefix}_results_{timestamp_str}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ ë¡œê·¸ íŒŒì¼ ì €ì¥ (ëˆ„ì  ê¸°ë¡)
        log_file = output_dir / "evaluation_log.txt"
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"í‰ê°€ ì‹œê°: {timestamp}\n")
            f.write(f"í‰ê°€ íƒ€ì…: {eval_type}\n")
            f.write(f"ëª¨ë¸ëª…: {model_name}\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_path.name}\n")
            f.write("-" * 80 + "\n")
            f.write(f"Precision (ì •ë°€ë„): {precision * 100:.2f}%\n")
            f.write(f"Recall (ì¬í˜„ìœ¨):    {recall * 100:.2f}%\n")
            f.write(f"F1 Score:           {f1 * 100:.2f}%\n")
            f.write(f"ì´ í† í° ìˆ˜:         {len(true_labels):,}\n")
            f.write(f"í‰ê°€ ì‹œê°„:          {results['evaluation_time']:.2f}ì´ˆ\n")
            
            # ì—”í‹°í‹°ë³„ ì„±ëŠ¥ (ìƒìœ„ 5ê°œ)
            if entity_metrics:
                f.write("\nì£¼ìš” ì—”í‹°í‹°ë³„ ì„±ëŠ¥:\n")
                sorted_entities = sorted(
                    entity_metrics.items(),
                    key=lambda x: x[1]['f1_score'],
                    reverse=True
                )[:5]
                for entity_type, metrics in sorted_entities:
                    f.write(f"  {entity_type}: F1={metrics['f1_score']:.2f}% "
                           f"P={metrics['precision']:.2f}% "
                           f"R={metrics['recall']:.2f}%\n")
            
            f.write("=" * 80 + "\n\n")
        
        if verbose:
            print(f"\nâœ“ JSON ê²°ê³¼ ì €ì¥: {json_file.name}")
            print(f"âœ“ í‰ê°€ ë¡œê·¸ ì €ì¥: {log_file} (ëˆ„ì )")
            print(f"âœ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    
    if verbose:
        print(f"\nâ±ï¸  í‰ê°€ ì‹œê°„: {results['evaluation_time']:.2f}ì´ˆ")
        print("=" * 60)
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì‚¬ìš© ì˜ˆì œ"""
    print("NER ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # 1. ì—”í‹°í‹° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    test_text = """ì €ì‘ë¬¼ ì €ì‘ì¬ì‚°ê¶Œ ì–‘ë„ ê³„ì•½ì„œ

ê³„ì•½ì: ê¹€ë¯¼ìˆ˜
ì „í™”ë²ˆí˜¸: 010-1234-5678
ì´ë©”ì¼: minsu.kim@gmail.com
ì£¼ì†Œ: ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123

ìˆ˜íƒê¸°ê´€: í•œêµ­ì½˜í…ì¸ ì§„í¥ì›
ë‹´ë‹¹ì: ë°•ì˜í¬ ë¶€ì¥
ê³„ì•½ê¸ˆ: 5,000,000ì›"""
    
    entities = extract_entities_from_text(test_text, debug=True)
    
    print(f"\nì¶”ì¶œëœ ì—”í‹°í‹° ({len(entities)}ê°œ):")
    for entity, label in entities:
        print(f"  - {entity} ({label})")
    
    # 2. í›ˆë ¨ ìƒíƒœ í™•ì¸
    print(f"\ní˜„ì¬ ëª¨ë¸ ìƒíƒœ:")
    status = get_training_status()
    print(f"  - ëª¨ë¸ ì¡´ì¬: {status['model_exists']}")
    print(f"  - ê²½ë¡œ: {status['model_path']}")
    if status.get('checkpoints', 0) > 0:
        print(f"  - ì²´í¬í¬ì¸íŠ¸: {status['checkpoints']}ê°œ")

if __name__ == "__main__":
    main()