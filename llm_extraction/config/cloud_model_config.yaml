# Cloud-based Model Configuration for Korean Metadata Extraction
# Alternative to local model storage using cloud APIs

# Cloud Configuration
cloud:
  enabled: true
  fallback_to_local: true  # Fallback to local models if cloud fails
  timeout: 30  # seconds
  retry_attempts: 3

# Cloud Providers Configuration
providers:
  huggingface:
    enabled: true
    base_url: "https://api-inference.huggingface.co/models"
    api_key_env: "HUGGINGFACE_API_KEY"
    timeout: 30
    retry_attempts: 3
    
  openai:
    enabled: true
    base_url: "https://api.openai.com/v1/chat/completions"
    api_key_env: "OPENAI_API_KEY"
    timeout: 30
    retry_attempts: 3
    
  together:
    enabled: true
    base_url: "https://api.together.xyz/v1/chat/completions"
    api_key_env: "TOGETHER_API_KEY"
    timeout: 30
    retry_attempts: 3
    
  anthropic:
    enabled: false
    base_url: "https://api.anthropic.com/v1/messages"
    api_key_env: "ANTHROPIC_API_KEY"
    timeout: 30
    retry_attempts: 3

# Model Configurations for Cloud APIs
models:
  # Hugging Face Inference API Models
  solar_ko_cloud:
    name: "SOLAR-Ko-10.7B-Cloud"
    provider: "huggingface"
    model_id: "upstage/SOLAR-10.7B-Instruct-v1.0"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.0001  # USD per 1K tokens
    
  qwen_cloud:
    name: "Qwen2.5-Cloud"
    provider: "huggingface"
    model_id: "Qwen/Qwen2.5-7B-Instruct"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.0001
    
  # OpenAI API Models
  gpt4o_mini:
    name: "GPT-4o-Mini"
    provider: "openai"
    model_id: "gpt-4o-mini"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.00015
    
  gpt4o:
    name: "GPT-4o"
    provider: "openai"
    model_id: "gpt-4o"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.005
    
  # Together AI Models
  llama2_7b:
    name: "Llama-2-7B-Chat"
    provider: "together"
    model_id: "meta-llama/Llama-2-7b-chat-hf"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.0002
    
  mistral_7b:
    name: "Mistral-7B-Instruct"
    provider: "together"
    model_id: "mistralai/Mistral-7B-Instruct-v0.1"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.0002
    
  qwen2_5_7b:
    name: "Qwen2.5-7B-Instruct"
    provider: "together"
    model_id: "Qwen/Qwen2.5-7B-Instruct"
    max_length: 4096
    temperature: 0.1
    top_p: 0.8
    cost_per_token: 0.0002

# Cost Optimization Settings
cost_optimization:
  enabled: true
  budget_limit: 100.0  # USD per month
  preferred_providers: ["huggingface", "together"]  # Cheaper options first
  fallback_providers: ["openai"]  # More expensive fallback
  
# Performance Settings
performance:
  batch_size: 10  # Process multiple requests together
  concurrent_requests: 5  # Max concurrent API calls
  cache_responses: true  # Cache API responses locally
  cache_duration: 3600  # seconds (1 hour)

# Monitoring and Logging
monitoring:
  enabled: true
  log_api_calls: true
  log_costs: true
  log_performance: true
  alert_thresholds:
    cost_per_hour: 10.0  # USD
    error_rate: 0.05  # 5%
    response_time: 10.0  # seconds
