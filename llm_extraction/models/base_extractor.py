#!/usr/bin/env python3
"""
Base LLM Extractor for Korean Document Metadata Extraction
Supports multiple open-source LLM models with JSON schema-based extraction
"""

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from abc import ABC, abstractmethod
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    pipeline
)
from pydantic import BaseModel, Field
import time

# Import model cache manager
from .model_cache import ModelCacheManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExtractionResult(BaseModel):
    """Standardized extraction result format"""
    document_type: str
    metadata: Dict[str, Any]
    confidence: float = Field(ge=0.0, le=1.0)
    extraction_time: float
    model_used: str
    raw_response: Optional[str] = None
    error: Optional[str] = None

class BaseLLMExtractor(ABC):
    """Abstract base class for LLM-based metadata extraction"""
    
    def __init__(self, model_config: Dict[str, Any], device: str = "auto", config_path: str = "config/model_config.yaml"):
        self.model_config = model_config
        self.device = device
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.cache_manager = ModelCacheManager(config_path)
        self._load_model()
    
    @abstractmethod
    def _load_model(self):
        """Load the specific model implementation"""
        pass
    
    @abstractmethod
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using the loaded model"""
        pass
    
    def _create_prompt(self, text: str, schema: Dict[str, Any], document_type: str) -> str:
        """Create a structured prompt for metadata extraction"""
        schema_str = json.dumps(schema, ensure_ascii=False, indent=2)
        
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ(ê³„ì•½ì„œ, ë™ì˜ì„œ, ê¸°íƒ€)ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…Â·ì£¼ì„Â·ë§ˆí¬ë‹¤ìš´Â·ì½”ë“œë¸”ë¡ ê¸ˆì§€.

ë‹¤ìŒì€ {document_type} ë¬¸ì„œì˜ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ JSON ìŠ¤í‚¤ë§ˆì— ë”°ë¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ í…ìŠ¤íŠ¸:
{text}

ì¶”ì¶œí•  ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ:
{schema_str}

ì§€ì‹œì‚¬í•­:
1. í…ìŠ¤íŠ¸ì—ì„œ ê° í•„ë“œì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ì•„ ì¶”ì¶œí•˜ì„¸ìš”
2. ì •ë³´ê°€ ëª…ì‹œì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ê²½ìš° ë°˜ë“œì‹œ nullì„ ì‚¬ìš©í•˜ì„¸ìš” (ì¶”ì¸¡ ê¸ˆì§€)
3. ë‚ ì§œëŠ” YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”
4. ê¸ˆì•¡ì€ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì„¸ìš” (ë‹¨ìœ„ ì œì™¸)
5. ì „í™”ë²ˆí˜¸ëŠ” ìˆ«ìì™€ í•˜ì´í”ˆ(-)ë§Œ í¬í•¨í•˜ì„¸ìš”
6. ì£¼ì†ŒëŠ” ì „ì²´ ì£¼ì†Œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”
7. ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ëŠ” ìˆ«ìì™€ í•˜ì´í”ˆ(-)ë§Œ í¬í•¨í•˜ì„¸ìš”
8. ì²´í¬ë°•ìŠ¤ ì •ë³´ ì²˜ë¦¬:
   - ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ëœ ìƒíƒœ(ğŸ“§, â˜‘, âœ“, â– , â—, â—¼, â—‰)ì¸ ê²½ìš° trueë¡œ ì„¤ì •
   - ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ë˜ì§€ ì•Šì€ ìƒíƒœ(â˜, â–¡, â—‹, â—¯, â—», â—¦)ì¸ ê²½ìš° falseë¡œ ì„¤ì •
   - ì²´í¬ë°•ìŠ¤ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ì²˜ë¦¬
   - OCR ì˜¤ë¥˜ ê³ ë ¤: "ëª©ì œê¶Œ"ì€ "ë³µì œê¶Œ"ìœ¼ë¡œ í•´ì„
9. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
10. ì¶”ê°€ ì •ë³´ë‚˜ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
11. ```jsonì´ë‚˜ ``` ê°™ì€ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì‚¬ìš© ê¸ˆì§€

ì‘ë‹µ (JSONë§Œ):

---
You are a helper that extracts information from Korean documents (contracts, consent forms, other documents).
Output only valid JSON. No explanations, comments, markdown, or code blocks allowed.

The following is OCR text from a {document_type} document. Extract metadata according to the given JSON schema.

Document text:
{text}

Metadata schema to extract:
{schema_str}

Instructions:
1. Find and extract information corresponding to each field in the text
2. Use null if information is not explicitly present or unclear (no guessing policy)
3. Convert dates to YYYY-MM-DD format
4. Extract only numbers for amounts (exclude units)
5. Include only numbers and hyphens(-) for phone numbers
6. Extract complete addresses accurately
7. Include only numbers and hyphens(-) for registration numbers
8. Checkbox information processing:
   - Set true for checked checkboxes (ğŸ“§, â˜‘, âœ“, â– , â—, â—¼, â—‰)
   - Set false for unchecked checkboxes (â˜, â–¡, â—‹, â—¯, â—», â—¦)
   - Automatically detect checkbox patterns for consistent processing
   - Consider OCR errors: "ëª©ì œê¶Œ" should be interpreted as "ë³µì œê¶Œ"
9. Respond only with valid JSON format
10. Do not include additional information or explanations
11. Do not use ```json or ``` markdown syntax

Response (JSON only):"""
        
        return prompt
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response and extract JSON"""
        try:
            # Clean up the response - remove markdown formatting
            cleaned_response = response.strip()
            
            # Remove markdown code blocks
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]  # Remove ```json
            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]   # Remove ```
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]  # Remove trailing ```
            
            # Split by ```json to get the first complete JSON block
            json_blocks = cleaned_response.split('```json')
            if len(json_blocks) > 1:
                # Take the first complete JSON block
                first_block = json_blocks[1].split('```')[0]
                cleaned_response = first_block.strip()
            
            # Try to find JSON in the response
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = cleaned_response[start_idx:end_idx]
                parsed_json = json.loads(json_str)
                
                # Validate that we got meaningful data
                if parsed_json and len(parsed_json) > 0:
                    return parsed_json
                else:
                    logger.warning("Empty JSON object found in response")
                    return {}
            else:
                logger.warning("No JSON found in response")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {e}")
            logger.error(f"Response: {response[:500]}...")  # Log first 500 chars
            return {}
        except Exception as e:
            logger.error(f"Unexpected error parsing response: {e}")
            logger.error(f"Response: {response[:500]}...")
            return {}
    
    def _calculate_confidence(self, metadata: Dict[str, Any], schema: Dict[str, Any]) -> float:
        """Calculate extraction confidence based on completeness"""
        if not metadata:
            return 0.0
        
        total_fields = len(schema.get('properties', {}))
        filled_fields = sum(1 for v in metadata.values() if v is not None and v != "")
        
        if total_fields == 0:
            return 1.0
        
        return min(filled_fields / total_fields, 1.0)

class SOLARKoExtractor(BaseLLMExtractor):
    """SOLAR-Ko model extractor"""
    
    def _load_model(self):
        """Load SOLAR-Ko model using cache manager"""
        model_id = self.model_config['model_id']
        max_length = self.model_config.get('max_length', 4096)
        
        logger.info(f"Loading SOLAR-Ko model: {model_id}")
        
        try:
            # Get cached model path
            model_path = self.cache_manager.get_model_path('primary')
            
            logger.info(f"Using model from cache: {model_path}")
            
            # Load tokenizer from cached path
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # Configure quantization for memory efficiency
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # Load model from cached path
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map=self.device,
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
            
            # Create pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=max_length,
                temperature=self.model_config.get('temperature', 0.1),
                top_p=self.model_config.get('top_p', 0.9),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            logger.info("SOLAR-Ko model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load SOLAR-Ko model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using SOLAR-Ko"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Generate response
            response = self.pipeline(
                prompt,
                max_new_tokens=1024,
                num_return_sequences=1,
                truncation=True
            )[0]['generated_text']
            
            # Extract the generated part (remove prompt)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            
            # Calculate confidence
            confidence = self._calculate_confidence(metadata, schema)
            
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Qwen2.5-7B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Qwen2.5-7B",
                error=str(e)
            )

class QwenExtractor(BaseLLMExtractor):
    """Qwen2.5 model extractor"""
    
    def _load_model(self):
        """Load Qwen2.5 model using cache manager"""
        model_id = self.model_config['model_id']
        max_length = self.model_config.get('max_length', 4096)
        
        logger.info(f"Loading Qwen2.5 model: {model_id}")
        
        try:
            # Get cached model path
            model_path = self.cache_manager.get_model_path('secondary')
            
            logger.info(f"Using model from cache: {model_path}")
            
            # Load tokenizer from cached path
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # Configure quantization
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # Load model from cached path
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map=self.device,
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
            
            # Create pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=max_length,
                temperature=self.model_config.get('temperature', 0.1),
                top_p=self.model_config.get('top_p', 0.9),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            logger.info("Qwen2.5 model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Qwen2.5 model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Qwen2.5"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Generate response
            response = self.pipeline(
                prompt,
                max_new_tokens=1024,
                num_return_sequences=1,
                truncation=True
            )[0]['generated_text']
            
            # Extract the generated part
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            
            # Calculate confidence
            confidence = self._calculate_confidence(metadata, schema)
            
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Qwen2.5-7B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Qwen2.5-7B",
                error=str(e)
            )

class LlamaExtractor(BaseLLMExtractor):
    """Llama 3.1-70B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Llama 3.1-70B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Llama 3.1-70B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            logger.info("Llama 3.1-70B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Llama 3.1-70B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Llama 3.1-70B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.model_config.get('max_length', 8192),
                truncation=True
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=self.model_config.get('temperature', 0.1),
                    top_p=self.model_config.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            confidence = self._calculate_confidence(metadata, schema)
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Llama-3.1-70B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Llama-3.1-70B",
                error=str(e)
            )

class Qwen72BExtractor(BaseLLMExtractor):
    """Qwen 2.5-72B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        self.pipeline = None
        
    def _load_model(self):
        """Load Qwen 2.5-72B model"""
        self.load_model('qwen72b')
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Qwen 2.5-72B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Qwen 2.5-72B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            # Create pipeline for consistency with other extractors
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=self.model_config.get('max_length', 8192),
                temperature=self.model_config.get('temperature', 0.1),
                top_p=self.model_config.get('top_p', 0.9),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            logger.info("Qwen 2.5-72B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Qwen 2.5-72B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Qwen 2.5-72B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Generate response using pipeline
            response = self.pipeline(
                prompt,
                max_new_tokens=1024,
                num_return_sequences=1,
                truncation=True,
                temperature=self.model_config.get('temperature', 0.1),
                top_p=self.model_config.get('top_p', 0.9),
                do_sample=True
            )[0]['generated_text']
            
            # Extract the generated part (remove prompt)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            
            # Calculate confidence
            confidence = self._calculate_confidence(metadata, schema)
            
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Qwen2.5-72B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Qwen2.5-72B",
                error=str(e)
            )

class QwenVLExtractor(BaseLLMExtractor):
    """Qwen 2.5-VL-72B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def _load_model(self):
        """Load Qwen 2.5-VL-72B model"""
        self.load_model('qwenvl')
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Qwen 2.5-VL-72B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Qwen 2.5-VL-72B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            logger.info("Qwen 2.5-VL-72B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Qwen 2.5-VL-72B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Qwen 2.5-VL-72B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.model_config.get('max_length', 8192),
                truncation=True
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=self.model_config.get('temperature', 0.1),
                    top_p=self.model_config.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            confidence = self._calculate_confidence(metadata, schema)
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Qwen2.5-VL-72B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Qwen2.5-VL-72B",
                error=str(e)
            )

class Qwen3Extractor(BaseLLMExtractor):
    """Qwen3-4B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def _load_model(self):
        """Load Qwen3-4B model"""
        self.load_model('qwen3')
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Qwen3-4B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Qwen3-4B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            logger.info("Qwen3-4B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Qwen3-4B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Qwen3-4B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.model_config.get('max_length', 4096),
                truncation=True
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=self.model_config.get('temperature', 0.1),
                    top_p=self.model_config.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            confidence = self._calculate_confidence(metadata, schema)
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Qwen3-4B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Qwen3-4B",
                error=str(e)
            )

class Gemma3Extractor(BaseLLMExtractor):
    """Gemma 3 12B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def _load_model(self):
        """Load Gemma 3 12B model"""
        self.load_model('gemma3_12b')
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Gemma 3 12B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Gemma 3 12B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            logger.info("Gemma 3 12B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Gemma 3 12B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Gemma 3 12B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.model_config.get('max_length', 131072),
                truncation=True
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=self.model_config.get('temperature', 0.1),
                    top_p=self.model_config.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            confidence = self._calculate_confidence(metadata, schema)
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Gemma3-12B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Gemma3-12B",
                error=str(e)
            )

class MixtralExtractor(BaseLLMExtractor):
    """Mixtral 8x7B model extractor"""
    
    def __init__(self, model_config: Dict[str, Any]):
        super().__init__(model_config)
        self.model = None
        self.tokenizer = None
        self.device = None
        
    def _load_model(self):
        """Load Mixtral 8x7B model"""
        self.load_model('mixtral_8x7b')
        
    def load_model(self, model_id: str, local_path: str = None):
        """Load Mixtral 8x7B model using cache manager"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            logger.info(f"Loading Mixtral 8x7B model: {model_id}")
            
            # Use cache manager if available
            if hasattr(self, 'cache_manager') and self.cache_manager:
                model_path = self.cache_manager.get_model_path(model_id)
            else:
                model_path = model_id
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set device
            self.device = next(self.model.parameters()).device
            logger.info(f"Device set to use {self.device}")
            
            logger.info("Mixtral 8x7B model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Mixtral 8x7B model: {e}")
            raise
    
    def extract_metadata(self, text: str, schema: Dict[str, Any], document_type: str) -> ExtractionResult:
        """Extract metadata using Mixtral 8x7B"""
        start_time = time.time()
        
        try:
            # Create prompt
            prompt = self._create_prompt(text, schema, document_type)
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.model_config.get('max_length', 32768),
                truncation=True
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=self.model_config.get('temperature', 0.1),
                    top_p=self.model_config.get('top_p', 0.9),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # Parse response
            metadata = self._parse_response(generated_text)
            confidence = self._calculate_confidence(metadata, schema)
            extraction_time = time.time() - start_time
            
            return ExtractionResult(
                document_type=document_type,
                metadata=metadata,
                confidence=confidence,
                extraction_time=extraction_time,
                model_used="Mixtral-8x7B",
                raw_response=generated_text
            )
            
        except Exception as e:
            logger.error(f"Error during metadata extraction: {e}")
            return ExtractionResult(
                document_type=document_type,
                metadata={},
                confidence=0.0,
                extraction_time=time.time() - start_time,
                model_used="Mixtral-8x7B",
                error=str(e)
            )

def create_extractor(model_name: str, config_path: str = "config/model_config.yaml") -> BaseLLMExtractor:
    """Factory function to create appropriate extractor"""
    
    # Load configuration
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if model_name.lower() == "solar-ko" or model_name.lower() == "solar":
        return SOLARKoExtractor(config['models']['primary'])
    elif model_name.lower() == "qwen" or model_name.lower() == "qwen2.5":
        return QwenExtractor(config['models']['secondary'])
    elif model_name.lower() == "lightweight" or model_name.lower() == "solar-ko-1.7b":
        return SOLARKoExtractor(config['models']['lightweight'])
    elif model_name.lower() == "llama" or model_name.lower() == "llama3.1":
        return LlamaExtractor(config['models']['llama'])
    elif model_name.lower() == "qwen72b" or model_name.lower() == "qwen2.5-72b":
        return Qwen72BExtractor(config['models']['qwen72b'])
    elif model_name.lower() == "qwenvl" or model_name.lower() == "qwen2.5-vl":
        return QwenVLExtractor(config['models']['qwenvl'])
    elif model_name.lower() == "qwen3" or model_name.lower() == "qwen3-4b":
        return Qwen3Extractor(config['models']['qwen3'])
    elif model_name.lower() == "qwen3-next" or model_name.lower() == "qwen3-next-80b":
        return Qwen3Extractor(config['models']['qwen3_next_80b'])
    elif model_name.lower() == "qwen3-30b":
        return Qwen3Extractor(config['models']['qwen3_30b'])
    elif model_name.lower() == "qwen3-235b":
        return Qwen3Extractor(config['models']['qwen3_235b'])
    elif model_name.lower() == "gemma3" or model_name.lower() == "gemma3-12b":
        return Gemma3Extractor(config['models']['gemma3_12b'])
    elif model_name.lower() == "mixtral" or model_name.lower() == "mixtral-8x7b":
        return MixtralExtractor(config['models']['mixtral_8x7b'])
    else:
        raise ValueError(f"Unsupported model: {model_name}")

if __name__ == "__main__":
    # Test the extractor
    extractor = create_extractor("solar-ko")
    
    test_text = """
    ì €ì‘ì¬ì‚°ê¶Œ ë¹„ë…ì ì  ì´ìš©í—ˆë½ ê³„ì•½ì„œ
    
    ì €ì‘ì ë° ì €ì‘ê¶Œ ì´ìš©í—ˆë½ì ì§‘ê±´ì— (ì´í•˜ "ê¶Œë¦¬ì" ì´ë¼ í•¨)ì™€ 
    ì €ì‘ê¶Œ ì´ìš©ì êµ­ë¦½ìƒíƒœì› ë©¸ì¢…ìœ„ê¸°ì¢…ë³µì›ì„¼í„°(ì´í•˜ "ì´ìš©ì" ì´ë¼ í•¨)ëŠ” 
    ì•„ë˜ ì €ì‘ë¬¼ ë©¸ì¢…ìœ„ê¸° ì•¼ìƒìƒë¬¼ ëŒ€êµ­ë¯¼ ì˜¨ë¼ì¸ í™ë³´ë¬¼ ì œì‘ì— ê´€í•œ 
    ì €ì‘ì¬ì‚°ê¶Œ ì´ìš©í—ˆë½ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì•½ì„ ì²´ê²°í•œë‹¤.
    """
    
    test_schema = {
        "type": "object",
        "properties": {
            "contract_type": {"type": "string", "description": "ê³„ì•½ì„œ ìœ í˜•"},
            "rights_holder": {"type": "string", "description": "ê¶Œë¦¬ì"},
            "user": {"type": "string", "description": "ì´ìš©ì"},
            "work_title": {"type": "string", "description": "ì €ì‘ë¬¼ ì œëª©"}
        }
    }
    
    result = extractor.extract_metadata(test_text, test_schema, "ê³„ì•½ì„œ")
    print(f"Extraction Result: {result}")
